?rdirichlet
knitr::opts_chunk$set(echo = TRUE)
require(DirichletReg) # We'll be using Dirichlet distributions. gtools includes rdirichlet and ddirichlet but not with log densities, which are useful.
?rdirichlet
?ddirichlet
exp(-9.47 - (-11.16))
plogp <- function(p) {
# bits, not nats
if(p==0) 0 else p*log2(p) # Consider why we can't just use p*log2(p).
}
cat_entropy <- function(catv) {
-sum(sapply(catv,plogp))
}
?permn
require(combinat) # From googling "all list permutations in R"
probs <- c(0,.2,.3,.75)
hyps <- permn(probs)
View(hyps)
?permn
prior <- c((1/24), rep 24) # Replace this with a prior over all hypotheses
cat_entropy(prior)
plogp <- function(p) {
# bits, not nats
if(p==0) 0 else p*log2(p) # Consider why we can't just use p*log2(p). Because log 0 = inf
}
cat_entropy <- function(catv) {
-sum(sapply(catv,plogp))
}
require(combinat) # From googling "all list permutations in R"
probs <- c(0,.2,.3,.75)
hyps <- permn(probs) # Generates all permutations, where order doesn't matter it is 4x3x2x1
prior <- c((1/24), rep 24) # Replace this with a prior over all hypotheses
cat_entropy(prior)
prior <- rep(1/24, nrow(hyps)) # Replace this with a prior over all hypotheses
cat_entropy(prior)
plogp <- function(p) {
# bits, not nats
if(p==0) 0 else p*log2(p) # Consider why we can't just use p*log2(p). Because log 0 = inf
}
cat_entropy <- function(catv) {
-sum(sapply(catv,plogp))
}
require(combinat) # From googling "all list permutations in R"
probs <- c(0,.2,.3,.75)
hyps <- permn(probs) # Generates all permutations, where order doesn't matter it is 4x3x2x1
prior <- rep(1/24, nrow(hyps)) # Replace this with a prior over all hypotheses
cat_entropy(prior)
prior <- rep(1/24, nrow(hyps)) # Replace this with a prior over all hypotheses
View(hyps)
prior <- rep(1/24) # Replace this with a prior over all hypotheses
cat_entropy(prior)
View(cat_entropy)
knitr::opts_chunk$set(echo = TRUE)
require(DirichletReg) # We'll be using Dirichlet distributions. gtools includes rdirichlet and ddirichlet but not with log densities.
plogp <- function(p) {
# bits, not nats
if(p==0) 0 else p*log2(p) # Consider why we can't just use p*log2(p). Because log 0 = inf
}
cat_entropy <- function(catv) {
-sum(sapply(catv,plogp))
}
require(combinat) # From googling "all list permutations in R"
probs <- c(0,.2,.3,.75)
hyps <- permn(probs) # Generates all permutations, where order doesn't matter it is 4x3x2x1
prior <- rep(1/24) # Entropy if we know nothing is 0.19
cat_entropy(prior)
update_probs <- function(prior,choice,outcome) {
# x, below, is a list of probabilities under a specific hypothesis
#Likelihood function is a zero for all hypotheses incompatible with the outcome
#and 1 otherwise
likelihood <- function(this_hyp,this_choice,this_outcome) {
if (this_outcome==this_choice){
likelihood[this_hyp] <- 1
} else {
likelihood <- 0
}
}
likes <- sapply(hyps,function(h) {likelihood(h,choice,outcome)})
unnp <- likes*prior
z <- sum(unnp)
unnp/z
}
post <- update_probs(prior,choice=1,outcome=0)
print(post)
update_probs <- function(prior,choice,outcome) {
# x, below, is a list of probabilities under a specific hypothesis
#Likelihood function is a zero for all hypotheses incompatible with the outcome
#and 1 otherwise
likelihood <- function(this_hyp,this_choice,this_outcome) {
prob_this_outcome <- (this_hyp[this_choice]*this_outcome) + (1-this_hyp[this_choice]*(1-this_outcome))
}
likes <- sapply(hyps,function(h) {likelihood(h,choice,outcome)})
unnp <- likes*prior
z <- sum(unnp)
unnp/z
}
post <- update_probs(prior,choice=1,outcome=0)
print(post)
cat_entropy(post)
log(24)
log2(24)
# First we want the probability of each outcome in each hyp
# Then we take the weighted sum over hypothesis probabilities
expected_outcome <- function(i,ph) {
sum(hyps[i])*ph
}
print(expected_outcome(2,post))
# First we want the probability of each outcome in each hyp
# Then we take the weighted sum over hypothesis probabilities
expected_outcome <- function(i,ph) {
sum(sapply(hyps, function(hyp) {hyp[i]})*ph) # the sapply results in a vector of 24. sapply loops over column and applies a function to each element
}
print(expected_outcome(2,post))
expected_entropy <- function(choice,p_hyps) {
pwin <- expected_outcome(choice,p_hyps)
cat_entropy(pwin)
}
print(expected_entropy(2,post))
#For steph 2
library(igraph)
library(tidyverse)
nodes<-c('Preference','Character','Closer','Knowledge','Visible','Choice')
graph<-matrix(c(0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,1,
0,0,0,0,0,0), ncol = 6, byrow=T)
G<-graph.adjacency(graph)
View(G)
V(G)$label<-V(G)$name<-nodes
V(G)$size <- 50
V(G)$color <- 'white'
E(G)$color <- 'black'
E(G)$width <- 2
plot(G)
?igraph
V(G)$color <- 'blue'
E(G)$color <- 'black'
E(G)$width <- 2
plot(G)
V(G)$color <- 'white'
E(G)$color <- 'red'
E(G)$width <- 2
plot(G)
#I figured its easier to think about in the form of a data frame
#So we have a column indexing the state for each cause using a factor (0,1)
#And a column for the probabilities for each unique combination
pChoice<-data.frame(expand.grid(list(Preference=c(0,1),
Character=c(0,1),
Closer=c(0,1),
Knowledge=c(0,1),
Visible = c(0,1)))) %>%
mutate(Preference = factor(Preference, levels = c(0,1),
labels = c('Absent','Hotdog')),
Character = factor(Character, levels = c(0,1),
labels = c('Lazy','Sporty')),
Closer = factor(Closer, levels = c(0,1),
labels = c('Pizza','Hotdog')),
Knowledge = factor(Knowledge, levels = c(0,1),
labels = c('No','Yes')),
Visible = factor(Visible, levels = c(0,1),
labels = c('Pizza','Hotdog')),
p_choose_hotdog = NA)
View(pChoice)
head(pChoice)
#Default preference for hotdog
baserate<-.5
#Strength of preference, strength of character
strengths<-list(preference=.5, character=.5)
#preference and character match only promote hotdog choice so long as the person either
#(a) knows the area or (b) can see the hotdog
tmp1<- pChoice$Knowledge=='Yes' | pChoice$Visible=='Hotdog'
#character match only promotes pizza choice so long as the person
#(a) knows the area or (b) can see the hotdog
tmp2<-pChoice$Knowledge=='Yes' | pChoice$Visible=='Pizza'
pChoice$p_choose_hotdog<-(1-(1-baserate) *
(1-strengths[['preference']]*as.numeric(pChoice$Preference=='Hotdog')*tmp1) *
#preference for hotdog pushes toward hotdog
(1-strengths[['character']]* as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Hotdog' |
#character match to distance to hotdog pushes toward hotdog
pChoice$Character=='Sporty' & pChoice$Closer=='Pizza')*tmp1)) *
#end of the noisy OR of generative causes
(1-strengths[['character']] * as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Pizza' |
#character match to distance to pizza pushes toward pizza
pChoice$Character=='Sporty' & pChoice$Closer=='Hotdog') * tmp2)
View(pChoice)
#Here's how it looks
pChoice
#Anc here's an example of using it to maker a basic counterfactual explanation type model
#Let's suppose a person chooses pizza in situation 15
case<-cbind(pChoice[15,], Choice='Pizza')
case
#Was it reasonably to be expected given the situation (in this case yes)
p_actual<-1-case$p_choose_hotdog
?unlist
#What features of the situation contributed most to its being selected?
#counterfactual contrasts:
p_counterfactual<-1-unlist(c(pChoice %>% filter(Preference!=case$Preference,
Character==case$Character,
Closer ==case$Closer,
Knowledge==case$Knowledge,
Visible==case$Visible) %>% select(preference=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character!=case$Character,
Closer ==case$Closer,
Knowledge==case$Knowledge,
Visible==case$Visible) %>% select(character=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character==case$Character,
Closer !=case$Closer,
Knowledge==case$Knowledge,
Visible==case$Visible) %>% select(closer=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character==case$Character,
Closer ==case$Closer,
Knowledge!=case$Knowledge,
Visible==case$Visible) %>% select(knowledge=p_choose_hotdog),
pChoice %>% filter(Preference==case$Preference,
Character==case$Character,
Closer ==case$Closer,
Knowledge==case$Knowledge,
Visible!=case$Visible) %>% select(visible=p_choose_hotdog)))
View(case)
dependence<-p_actual-p_counterfactual
#In this case we might reasonably blame her lazy character or the fact that the pizza was closer
Collapse
View(case)
View(pChoice)
dependence
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_push * char_push
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
pref_match_hotdog <- as.numeric(pChoice$Preference=='Hotdog')
char_match_hotdog <- as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Hotdog' |
pChoice$Character=='Sporty' & pChoice$Closer=='Pizza')
char_match_pizza <- as.numeric(pChoice$Character=='Lazy' & pChoice$Closer=='Pizza' |
#character match to distance to pizza pushes toward pizza
pChoice$Character=='Sporty' & pChoice$Closer=='Hotdog')
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
#preference and character match only promote hotdog choice so long as the person either
#(a) knows the area or (b) can see the hotdog ie generative ie things that make you want hotdog
prom_hotdog <- pChoice$Knowledge=='Yes' | pChoice$Visible=='Hotdog'
#character match only promotes pizza choice so long as the person
#(a) knows the area or (b) can see the hotdog ie preventative factor ie things that amke you want pizza
prom_pizza <- pChoice$Knowledge=='Yes' | pChoice$Visible=='Pizza'
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_push * char_push
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
# My meddling
hotdog_promotion <- function(char, pref, baserate) {
pref_prom_hotdog <- 1 - pref * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - char * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(0.5,0.5,0.5))
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(strengths[['character']], strengths[['preference']], baserate))
pChoice$prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate))
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate))
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion(c(strengths[['character']], strengths[['preference']], baserate)))
# My meddling
hotdog_promotion <- function() {
pref_prom_hotdog <- 1 - strengths[['preference']] * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - strengths[['character']] * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1-char*char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion())
# My meddling
hotdog_promotion <- function() {
pref_prom_hotdog <- 1 - strengths[['preference']] * pref_match_hotdog * prom_hotdog
char_prom_hotdog <- 1 - strengths[['character']] * char_match_hotdog * prom_hotdog
noisyOR <- 1-(1-baserate)
hotdog_score <- noisyOR * pref_prom_hotdog * char_prom_hotdog
pizza_score <- 1- strengths[['character']] *char_match_pizza * prom_pizza
p_choose_hotdog <- hotdog_score * pizza_score
return (p_choose_hotdog)
}
prob_choose_hotdog <- sapply(pChoice, hotdog_promotion())
22/110
2.002/10.10
5.005/10.01
2.005/10.1
2.002/10.1
52/110
exp(100)
exp(1)
runif(4)
sessionInfo()
sessionInfo()
sessionInfo()
install.packages("afex")
install.packages("faux")
library(broom, tidyverse, faux, afex)
library(broom)
library(faux)
library(afex)
R.version()
R.Version()
.libPaths()
.libPaths()
R.Version()
# ------- Prelims -----------
library(tidyverse)
library(ggplot2)
# ----------- Define an example prior df -------------------------
# Here define two causal vars and an exogenous noise variable for each (i.e. var epsilon A goes with A)
# in the exp setting this is 0.5
p_A <- c(.1,.9) # ie A usually has value 1... base rate for cause
p_epsA <- c(.7,.3) #... most of the time the noise var for a doesn't occur. for a to work it needs a and exp a. a is usually present but ogten doesnt work cos of noise term not working
p_B <- c(.8,.2) # B rarely fires 1...
p_epsB <- c(.3,.7) # but when it does it is strong
# And wrap them into a df called prior. Later the function should take dfs of this format:
# i.e. any number of causes as the rows, and the probs of them taking 0 and 1 as cols
params <- data.frame(rbind(p_A, p_epsA, p_B, p_epsB))
colnames(params) <- c(0,1)
# Other values set outside for now
N_cf <- 1000L # How many counterfactual samples to draw
s <- .7 # Stability
n_causes <- nrow(params)
causes <- rownames(params)
# Make a df of all combinations of variable settings
df <- expand.grid(rep(list(c(0,1)),n_causes), KEEP.OUT.ATTRS = F)
# ... with variables as the column names
colnames(df) <- causes
worlds <- nrow(df)
View(df)
structure <- 'disjunctive'
if (structure=="disjunctive") {
df$E <- as.numeric((df[1] & df[2]) | (df[3] & df[4]))
}
# Can replace with this - if rename - it is deterministic - literally gives specific outcome for set 3 causes, needs actual input. mechanical tell syou whether effects occurred given setting
# df$effect <- max( c(min(c1,e1), min(c2,e2), min(c3, e3), min(c2*c3, e23))) # BUT SAME PROBLEM - HOW TO AUTOMATICALLY DEAL WITH ANY NUMBER OF CAUSES?
mat <- as.matrix(df[,1:4])
View(mat)
# df2 <- as.matrix(df, dimnames=NULL)
# dimnames = list(c(1:16), c(causes))
# Replace every cell with the relevant indexed edge strength from params
for (k in 1:worlds){
for (cause in causes) {
a <- params[cause,df[k,cause]+1] # It needs the '+1' because r indexes from 1 not 0
mat[k,cause] <- a # ((then sometimes #*df[k,cause] if do at same time as structure but change later if need))
}
}
View(mat)
View(params)
# For each row of df, the prior is now the product of the same row of df2
df$Pr <- apply(mat, 1, prod) # parameter of the model
sum(df$Pr)
# Then loop to calculate cfs and assign causal responsibility
# Loop through possible world settings
for (c_ix in 1:worlds)
sd(3000)
?rbinom
N <- 10^4
rbinom(N,1,.3)
sd_v <- sd(v)
v <- rbinom(N,1,.3)
sd_v <- sd(v)
?sd
th <- 0.3*0.7
p <- 0.3
derived <- (N*p(1-p))^0.5
derived <- (N*p*(1-p))^0.5
derived <- (N*3000*(1-p))^0.5
(p*(1-p)/n)^0.5
(p*(1-p)/N)^0.5
((p*(1-p))/N)^0.5
sum(v)
((3050*(6950))/N)^0.5
(N*p*(1-p))^0.5
q <- 0.7
p*q
0.3*0.7
N*p*q
2100^0.5
sqrt(2100)
(p*q/N)^0.5
((p*q)/N)^0.5
0.198^0.5
pest <- mean(v)
var <- (pest*(1-pest))/N
sqrt(var)
n <- 100
p*q/n
sqrt(p*q/n)
v2 <- rbinom(n,1,.3)
sd(v2)
sqrt(p*q)
sd(pest*(1-pest))
n2 <- 100000
v3 <- rbinom(n2,1,.3)
sd(v3)
sd(v2)
sd(v)
sum(v)
mean(v)
r <- mean(v)
g <- 1-r
sqrt(r*g)
#------- 2. Create parameters, run cesm, get model predictions and save them ------------
source('setParams.R')
getwd()
setwd("~/Documents/GitHub/Collider_cogsci25/Main_scripts")
#------- 2. Create parameters, run cesm, get model predictions and save them ------------
source('setParams.R')
source('getModelPreds.R') #
library(tidyverse)
source('getModelPreds.R') #
# Load functions: world_combos, get_cfs
source('functionsN2.R')
load('../model_data/params.rdata', verbose = T) # defined in script `setParams.r`
#------- 2. Create parameters, run cesm, get model predictions and save them ------------
source('setParams.R')
source('getModelPreds.R') #
#######################################################
###### Collider - tidy up model predictions FULL  #####
#######################################################
# The functions from 'functions2' generated model predictions for CESM only.
# To make the full model and all the lesioned version, run this preprocessing step and then combine in 'modelCombLesions'
#library(tidyverse)
rm(list=ls())
all <- read.csv('../Model_data/all.csv') # 1440
all$pgroup <- as.factor(all$pgroup)
# Bring in trialtype and rename as the proper string name
all$trialtype <- all$groupPost
all$trialtype[all$trialtype==1 & all$structure=='disjunctive'] <- 'd1'
all$trialtype[all$trialtype==2 & all$structure=='disjunctive'] <- 'd2'
all$trialtype[all$trialtype==3 & all$structure=='disjunctive'] <- 'd3'
all$trialtype[all$trialtype==4 & all$structure=='disjunctive'] <- 'd4'
all$trialtype[all$trialtype==5 & all$structure=='disjunctive'] <- 'd5'
all$trialtype[all$trialtype==6 & all$structure=='disjunctive'] <- 'd6'
all$trialtype[all$trialtype==7 & all$structure=='disjunctive'] <- 'd7'
all$trialtype[all$trialtype==1 & all$structure=='conjunctive'] <- 'c1'
all$trialtype[all$trialtype==2 & all$structure=='conjunctive'] <- 'c2'
all$trialtype[all$trialtype==3 & all$structure=='conjunctive'] <- 'c3'
all$trialtype[all$trialtype==4 & all$structure=='conjunctive'] <- 'c4'
all$trialtype[all$trialtype==5 & all$structure=='conjunctive'] <- 'c5'
# First we have to average the model runs - goes from 1920 to 192
all <- all %>% group_by(pgroup, structure, index) %>%
mutate(A_cesm = mean(mA), Au_cesm = mean(mAu), B_cesm = mean(mB), Bu_cesm = mean(mBu)) %>%
distinct(pgroup, structure, index, .keep_all = TRUE)
# Pivot longer and list node names with their CESM values
all <- all %>% pivot_longer(cols = c(A_cesm:Bu_cesm), names_to = c('node', '.value'), names_sep = '_')
all <- all %>% select(-(mA:run))
# 768 is then 1920/10 = 192 x 4 variables
# The unobserved variables have different explanatory role depending what we presume their value to be.
# So we need to split them out. First one with 6 (just for unobserved)
all$node2 <- all$node
all$node[all$Au=='0' & all$node2=="Au"] <- 'Au=0'
all$node[all$Au=='1' & all$node2=="Au"] <- 'Au=1'
all$node[all$Bu=='0' & all$node2=="Bu"] <- 'Bu=0'
all$node[all$Bu=='1' & all$node2=="Bu"] <- 'Bu=1'
# Also need one with 8, where every node takes the value it has
all$node3 <- all$node
all$node3[all$A=='0' & all$node2=='A'] <- 'A=0'
all$node3[all$A=='1' & all$node2=='A'] <- 'A=1'
all$node3[all$B=='0' & all$node2=='B'] <- 'B=0'
all$node3[all$B=='1' & all$node2=='B'] <- 'B=1'
# Get a tag of the unobserved variables' settings. Then we can group data by this for plotting
all <- all %>% unite("uAuB", Au,Bu, sep= "", remove = FALSE)
# -------
# Also need a column for the actual settings
# They should be:
# c1: 000
# c2: 010
# c3: 100
# c4: 110
# c5: 111
# d1: 000
# d2: 010
# d3: 011
# d4: 100
# d5: 101
# d6: 110
# d7: 111
# write this as csv in case need it later - 576 rows because: 3 pgroups x 12 trialtypes x 4 nodes x 4 prior possible settings of unobserved variables
write.csv(all, '../Model_data/tidiedPreds.csv')
getwd()
source(knitr::purl('modelCombLesions.Rmd')) # puts the processed model predictions together with lesions to get a df called 'modelAndDataUnfit.csv'
View(modelAndData)
write.csv(modelAndData, '../Model_data/modelAndDataUnfit.csv')
View(modelAndData)
source(knitr::purl('optimise.Rmd')) # Get predictions and optimise: Get nll and tau for each model
source(knitr::purl('optimise.Rmd')) # Get predictions and optimise: Get nll and tau for each model
source(knitr::purl('optimise.Rmd')) # Get predictions and optimise: Get nll and tau for each model
View(df)
source(knitr::purl('reportingFigs.Rmd'))
source(knitr::purl('reportingFigs.Rmd'))
